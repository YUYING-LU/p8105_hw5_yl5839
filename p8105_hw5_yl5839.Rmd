---
title: "p8015_hw5_yl5839"
author: "Yuying Lu"
date: "2024-11-13"
output: github_document
---

```{r setup, include = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)

theme_set(theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Set function

```{r}
birth_share = function(group_size){
  birth_select = sample(1:365,group_size,replace=TRUE)
  return(length(unique(birth_select))<group_size)
}
```

Running this function for 10000 times for each group size between 2 and 50, calculate the probability of birthday sharing and make the plot.

```{r, fig.width=10, fig.height=5}
set.seed(1)

sim_results_df = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    birth_share = map(group_size, birth_share)
  ) |> 
  unnest(birth_share) |> 
  group_by(group_size) |> 
  summarise(probability = mean(birth_share))

sim_results_df |>
  mutate(
    group_size = str_c(group_size),
    group_size = fct_inorder(group_size)) |> 
  ggplot(aes(x = group_size, y = probability))+
  geom_point()+
  geom_line()
```

**Comment:** According to the picture based on the 10000 times simulation, we can see the probability that at least two people in the group will share a birthday increases as the group size increases. And the probability will converges to 1 when the group size is large.

# Problem 2

Generate 5000 datasets, each with $n=30$ samples from $N(\mu=0,\sigma=5)$ and or each data set, calculate the sample mean $\hat{\mu}$ and p-value of the hypothesis test with $\alpha = 0.05$.

```{r}
set.seed(2)
norm_df = 
  expand_grid(
    mu_true = 0,
    iter = 1:5000
  ) |> 
  mutate(
    norm_sample = map(mu_true, \(mu) rnorm(n = 30, mean = mu, sd = 5))
  ) |> 
  mutate(
  test_result = map(norm_sample, \(df) broom::tidy(t.test(df, mu = 0)))
) |> 
  unnest(test_result) |> 
  select(mu_true, iter, estimate, p.value)

norm_df 
```

The column named `estimate` represents $\hat{\mu}$.

Then repeat the above process for $\mu = \{1,2,3,4,5,6\}$:

```{r}
set.seed(3)
new_norm_df = 
  expand_grid(
    mu_true = 1:6,
    iter = 1:5000
  ) |> 
  mutate(
    norm_sample = map(mu_true, \(mu) rnorm(n = 30, mean = mu, sd = 5))
  ) |> 
  mutate(
  test_result = map(norm_sample, \(df) broom::tidy(t.test(df, mu = 0)))
) |> 
  unnest(test_result) |> 
  select(mu_true, iter, estimate, p.value)

```

Next we plot the proportion of times the null was rejected

```{r}
new_norm_df |> 
  mutate(reject = p.value < 0.05) |> 
  group_by(mu_true) |> 
  summarise(reject_proportion = mean(reject)) |> 
  ggplot(aes(x = mu_true, y = reject_proportion)) +
  geom_point() +
  geom_line() +
  theme_minimal(base_size = 15) +
  xlab(expression(mu)) +
  scale_x_continuous(breaks = 1:6)
  
```

**Association:** When $\mu \in \{1,...,6\}$, a larger $\mu$ has a higher reject proportion and it converges to 1.

Showing average $\hat{\mu}$ and the average $\hat{\mu}$ when null was rejected. v.s. true $\mu$

```{r, fig.width=6, fig.height=6}
new_norm_df |> 
  mutate(reject_mu = ifelse(p.value < 0.05, estimate, NA)) |> 
  group_by(mu_true) |> 
  summarise(average_mu = mean(estimate),
            average_mu_rejected = mean(reject_mu,na.rm = TRUE)) |> 
  pivot_longer(
    average_mu:average_mu_rejected,
    names_to = "type",
    values_to = "hat_mu"
  ) |> 
  ggplot(aes(x = mu_true, y = hat_mu, color = type))+
  geom_point(alpha = 0.75) +
  geom_abline(linetype = "dashed", size = .5, color = 'lightgrey') +
  theme_minimal() +
  xlab(expression(mu)) +
  ylab(expression(hat(mu))) +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = 1:6) +
  theme(legend.position = "bottom")
```

From the plot, the sample average of $\hat{\mu}$ across tests for which the null is rejected isn't approximately equal to the true value of $\mu$, when the true $\mu$ is close to $0$. The reason is when calculating the average of the estimate $\hat{\mu}$ in the sample that the null was rejected (we call it `average_mu_rejected`), we actually excluded the estimate $\hat{\mu}$ that is approximate to 0. By the Law of Large Number theorem, the average of all the estimates $\hat{\mu}$ (we call it `average_mu`) converges to the true $\mu$. However, when we exclude the estimates close to 0, the rest average increases, leading to a estimating bias. Additionally, when the true $\mu$ is close to 0, excluding the estimate that was rejected can result in a more significant deviation of the `average_mu_rejected` from the true $\mu$. This exclusion creates a larger bias, as illustrated in the figure.

# Problem 3

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homi_df = read_csv(url)
homi_df
```

The raw data gathers homicides in 50 larges U.S. cities, containing `r nrow(homi_df)` rows and `r ncol(homi_df)` columns. Each row represents a case of homicide including its uid, reported date, information of the victim (first and last name, race, age, sex), city and state where the homicide took place, the latitude and longitude of this place, and the disposition of this case.

### Data Wrangling

```{r}
summary_df = 
  homi_df |> 
  mutate(city_state = paste0(city, ', ', state)) |> 
  group_by(city_state) |> 
  summarise(
    total_num = n(),
    unsolved_num = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
summary_df
```

### `prop,test` for city of Baltimore, MD

```{r}
baltimore_df = 
  summary_df |> 
  filter(city_state == "Baltimore, MD") 

prop_test_output = 
  prop.test(baltimore_df$unsolved_num, baltimore_df$total_num)

tidy_df = 
  prop_test_output |> 
  broom::tidy() 

tidy_df |> 
  select(estimate, conf.low, conf.high)
```
The estimate proportion of unsolved homicides is `r pull(tidy_df, estimate)`, and its confidence interval is [`r pull(tidy_df, conf.low)`, `r pull(tidy_df, conf.high)`].

### `prop,test` for all cities

```{r}
prop_test_all = 
  summary_df |> 
  mutate(
    prop_test = map2(unsolved_num, total_num, \(un_n, n) prop.test(un_n, n)),
    prop_test = map(prop_test, broom::tidy)
    ) |> 
  unnest(prop_test) 

prop_test_all

prop_test_all|> 
  mutate(
    CI = paste0("[", round(conf.low,4),", ",round(conf.high,4),"]"),
    estimate = round(estimate,4)
  ) |> 
  select(city_state, estimate, CI)
```

Make the plot for the prop.test result:

```{r, fig.height= 12, fig.width=8}
prop_test_all |> 
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(size = 2, color = "#DF6604") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.3, color = "darkgray", size = 0.8) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18),  
    axis.title.x = element_text(size = 14, face = "bold"),  
    axis.title.y = element_text(size = 14, face = "bold"),  
    axis.text.x = element_text(size = 12, color = "black"), 
    axis.text.y = element_text(size = 12, color = "black"), 
    panel.grid.major = element_line(color = "lightgray", size = 0.5),  
    panel.grid.minor = element_blank()  
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))  
```

